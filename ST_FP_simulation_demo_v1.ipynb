{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMIT License\\n\\nCopyright (c) 2025 Ming Sun \\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2025 Ming Sun \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src/')) # Add the src/ folder to the system path\n",
    "import utils\n",
    "import numpy as np\n",
    "import stfp_func as bf\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimensions: (384, 384)\n",
      "Output dimensions: (384, 384)\n",
      "[Warning replace with your own data]\n"
     ]
    }
   ],
   "source": [
    "usaf = bf.read_mat('../data/', 'usaf_target')\n",
    "print('[Warning replace with your own data]')\n",
    "target_size = 784 \n",
    "pad_noise = (120 + 20 * torch.rand(target_size, target_size, dtype=usaf.dtype, device=usaf.device))\n",
    "pad_noise[200:200+384, 200:200+384] = usaf # Insert the original tensor into the center\n",
    "padded_usaf = pad_noise\n",
    "\n",
    "new_size1 = (1500, 1500)  \n",
    "am = bf.rescale_tensor(bf.add_gaussian_noise(bf.norm_rescale(torch.nn.functional.interpolate(padded_usaf.unsqueeze(0).unsqueeze(0), size=new_size1, mode='bilinear').squeeze()), mean=0.0, std=0.005), 0.01, 0.99)\n",
    "ph = -(am-0.5)*torch.pi/2\n",
    "Obj_full_view = am * torch.exp(1j*ph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define parameters\n",
    "T = 100  # Number of timestamps \n",
    "T_all = T + 4 \n",
    "H, W = Obj_full_view.shape[-2:] \n",
    "padding_size = 100  # To prevent edge artifacts\n",
    "# Define time-varying swirl strength (example: linearly increasing)\n",
    "half_linspace = torch.linspace(-0.5, 0.5, 40)\n",
    "half_linspace_ = torch.linspace(0.5, -0.5, 40)\n",
    "swirl_strengths = torch.cat([half_linspace, half_linspace_, half_linspace, torch.zeros(4)]) \n",
    "swirled_obj_3d = torch.zeros((T_all, H, W), dtype=torch.complex64, device=Obj_full_view.device)\n",
    "# Generate 3D tensor over time\n",
    "for t in range(T_all):\n",
    "    swirl_strength = swirl_strengths[t].item()\n",
    "    swirled_obj_3d[t] = bf.swirl2d(Obj_full_view, swirl_strength, padding_size)\n",
    "crop_row1, crop_row2 = 402, 1002\n",
    "crop_col1, crop_col2 = 450, 1050\n",
    "new_am_seq = torch.clamp(bf.norm_rescale_3Dslice(bf.histogram_matching(torch.clamp(bf.norm_rescale_3Dslice(swirled_obj_3d[:,crop_row1:crop_row2,crop_col1:crop_col2].abs()), min= 1e-2, max= 0.98), T_all-1)), min= 1e-2)\n",
    "new_ph_seq = -(new_am_seq-0.5)*torch.pi/2\n",
    "Obj_full_view_seq = new_am_seq * torch.exp(1j*new_ph_seq)\n",
    "del swirled_obj_3d, new_am_seq, new_ph_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Rreplace with your designed patterns\n"
     ]
    }
   ],
   "source": [
    "## load the pattern\n",
    "pattern = bf.read_cell_mat('../data/', 'pattern5x16p_v2')\n",
    "print('[Warning] Rreplace with your designed patterns')\n",
    "single_pattern = []\n",
    "for pattern_tmp in pattern: \n",
    "    single_pattern.append(pattern_tmp[0, :]) \n",
    "    single_pattern.append(pattern_tmp[1, :]) \n",
    "del pattern_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameter setting\n",
    "FP = bf.FP_parameter(\n",
    "    wavelength=0.53e-6,\n",
    "    pix=3e-6,  # Pixel size of the CCD\n",
    "    mag=5,\n",
    "    mag_image=3,\n",
    "    NA=0.14,\n",
    "    arraysize=14,\n",
    "    LEDgap=5e-3,\n",
    "    LEDheight=80e-3,\n",
    "    device=\"cuda\"  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx, bottom_idx = bf.pattern_dpc(2.0*FP.NA, FP, theta=0.0)\n",
    "left_idx, right_idx = bf.pattern_dpc(2.0*FP.NA, FP, theta=90.0)\n",
    "arraysize = FP.arraysize  \n",
    "num_leds = int(arraysize * arraysize)\n",
    "dpc_single_pattern = []\n",
    "for idx_group in [top_idx, bottom_idx, left_idx, right_idx]:\n",
    "    mask = torch.zeros(num_leds, dtype=torch.float64)\n",
    "    mask[idx_group.view(-1).long()] = 1.0  # Set 1 at active LED positions\n",
    "    dpc_single_pattern.append(mask) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## phase initialization in quasi static condition\n",
    "LR_data_ = bf.FP_forward(FP,Obj_full_view.squeeze(), device0=None) \n",
    "dpc_data = torch.zeros(int(len(dpc_single_pattern)), int(Obj_full_view.shape[0]/FP.mag_image), int(Obj_full_view.shape[1]/FP.mag_image), dtype=torch.float)\n",
    "for i in range(len(dpc_single_pattern)):\n",
    "    selected_index = torch.nonzero(dpc_single_pattern[i] == 1).squeeze(1)\n",
    "    dpc_data[i, :, :] = torch.sum(LR_data_[selected_index, :, :], dim=0)\n",
    "dpc_data[[0, 1], :, :] = dpc_data[[1, 0], :, :] \n",
    "mask_half_len = 20\n",
    "reg2 = 0.15\n",
    "phase_ref_full_view = bf.phase_gen_multi_pat(dpc_data,dpc_single_pattern,FP,mask_half_len,reg2)\n",
    "phase_ref_full_view = phase_ref_full_view*1\n",
    "del LR_data_, selected_index\n",
    "\n",
    "## amplitude initialization by near bright field (top+bottom)\n",
    "LR_data_ = bf.FP_forward(FP,Obj_full_view.squeeze(), device0=None) \n",
    "dpc_data = torch.zeros(int(len(dpc_single_pattern)), int(Obj_full_view.shape[0]/FP.mag_image), int(Obj_full_view.shape[1]/FP.mag_image), dtype=torch.float)\n",
    "for i in range(len(dpc_single_pattern)):\n",
    "    selected_index = torch.nonzero(dpc_single_pattern[i] == 1).squeeze(1)\n",
    "    dpc_data[i, :, :] = torch.sum(LR_data_[selected_index, :, :], dim=0)\n",
    "am_ref_full_view = torch.pow(torch.sqrt(1/4*torch.sum(dpc_data[:4], dim = 0)),1.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward multiplexed imaging\n",
    "raw_data_ = torch.zeros(int(T), int(Obj_full_view_seq.shape[1]/FP.mag_image), int(Obj_full_view_seq.shape[2]/FP.mag_image), dtype=torch.float)\n",
    "n_p = len(pattern) \n",
    "frame_num = len(single_pattern) // len(pattern) \n",
    "start_index = 0 \n",
    "num_time_slot = int(T/2)\n",
    "for i_ref in range(num_time_slot):\n",
    "    start_ref = start_index + i_ref* frame_num  \n",
    "    end_ref = start_ref + frame_num - 1\n",
    "    pattern_th = int(torch.ceil(torch.remainder(torch.tensor(start_ref + 1, dtype=torch.float32), \n",
    "                                            torch.tensor(n_p * frame_num, dtype=torch.float32)) / frame_num)) -1 \n",
    "    pattern_mat_tmp = pattern[pattern_th]\n",
    "    FP.led_pattern_seq = torch.nonzero(pattern_mat_tmp[0, :] != 0).squeeze()\n",
    "    LR_data = bf.FP_forward(FP,Obj_full_view_seq[start_ref].squeeze(), device0=None) \n",
    "    raw_data_[2*i_ref, :, :] = torch.sum(LR_data[FP.led_pattern_seq, :, :], dim=0)\n",
    "    FP.led_pattern_seq = torch.nonzero(pattern_mat_tmp[1, :] != 0).squeeze()\n",
    "    LR_data = bf.FP_forward(FP,Obj_full_view_seq[end_ref].squeeze(), device0=None) \n",
    "    raw_data_[2*i_ref+1, :, :] = torch.sum(LR_data[FP.led_pattern_seq, :, :], dim=0)\n",
    "del LR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing of the raw data\n",
    "raw_data__ = bf.temporal_gaussian_smooth_gpu(raw_data_, sigma_time=1.0, kernel_size=5, index_frame=2)\n",
    "[ss0,ss1,ss2] = raw_data_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate reference target over time (initial guess) \n",
    "obj_ref_3d = torch.zeros((T, raw_data_.shape[1], raw_data_.shape[2]), dtype=torch.complex64)\n",
    "am_ref_3d = torch.zeros((T, raw_data_.shape[1], raw_data_.shape[2]))\n",
    "phase_ref_3d = torch.zeros((T, raw_data_.shape[1], raw_data_.shape[2]))\n",
    "am_ref =  am_ref_full_view[int(crop_row1/FP.mag_image):int(crop_row2/FP.mag_image),int(crop_col1/FP.mag_image):int(crop_col2/FP.mag_image)]\n",
    "phase_ref = phase_ref_full_view[int(crop_row1/FP.mag_image):int(crop_row2/FP.mag_image),int(crop_col1/FP.mag_image):int(crop_col2/FP.mag_image)]\n",
    "\n",
    "am_ref_full_view_up_sampled = torch.clamp(bf.norm_rescale(torch.nn.functional.interpolate(am_ref_full_view.unsqueeze(0).unsqueeze(0), size=(int(FP.mag_image*am_ref_full_view.shape[0]),int(FP.mag_image*am_ref_full_view.shape[1])), mode='bilinear').squeeze()),0.01,0.99).cpu()\n",
    "phase_ref_full_view_up_sampled = bf.rescale_tensor(torch.nn.functional.interpolate(phase_ref_full_view.unsqueeze(0).unsqueeze(0), size=(int(FP.mag_image*phase_ref_full_view.shape[0]),int(FP.mag_image*phase_ref_full_view.shape[1])), mode='bilinear').squeeze(), phase_ref_full_view.min(), phase_ref_full_view.max())\n",
    "obj_ref_3d_full_view = torch.zeros((T, phase_ref_full_view_up_sampled.shape[0], phase_ref_full_view_up_sampled.shape[1]), dtype=torch.complex64)\n",
    "for t in range(T):\n",
    "    swirl_strength = swirl_strengths[t].item()\n",
    "    obj_ref_3d_t_upsample= bf.swirl2d((am_ref_full_view_up_sampled * torch.exp(1j*phase_ref_full_view_up_sampled.cpu())).cpu(), swirl_strength, 100).squeeze()\n",
    "    am_ref_3d[t] = torch.nn.functional.interpolate(obj_ref_3d_t_upsample.abs().unsqueeze(0).unsqueeze(0), size=(am_ref_full_view.shape[0],am_ref_full_view.shape[1]), mode='bilinear').squeeze()[int(crop_row1/FP.mag_image):int(crop_row2/FP.mag_image),int(crop_col1/FP.mag_image):int(crop_col2/FP.mag_image)]\n",
    "    phase_ref_3d[t] = torch.nn.functional.interpolate(obj_ref_3d_t_upsample.angle().unsqueeze(0).unsqueeze(0), size=(am_ref_full_view.shape[0],am_ref_full_view.shape[1]), mode='bilinear').squeeze()[int(crop_row1/FP.mag_image):int(crop_row2/FP.mag_image),int(crop_col1/FP.mag_image):int(crop_col2/FP.mag_image)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prior calculation: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "## generate abs&phase priors based on raw data and reference target over time\n",
    "n_p = len(pattern)\n",
    "frame_num = len(single_pattern) // len(pattern) \n",
    "start_index = 0 \n",
    "num_time_slot = int(ss0/2)\n",
    "frame_set_ = raw_data_[start_index:]\n",
    "huber_l1 = utils.HuberL1Params(warps=4, lamda=1e-3, theta=0.2, epsilon=0.1, \n",
    "                               max_iteration=4, nscales=4, sigma=0.6, zfactor=0.6, gscale=255)\n",
    "am_prior = torch.zeros(num_time_slot*2, raw_data_.shape[1], raw_data_.shape[2])\n",
    "phase_prior = torch.zeros(num_time_slot*2, raw_data_.shape[1], raw_data_.shape[2])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for i in tqdm(range(num_time_slot*2), desc = \"prior calculation\"):\n",
    "    ii = start_index + i\n",
    "    am_ref_ = am_ref_3d[i]\n",
    "    phase_ref_ = phase_ref_3d[i]\n",
    "    rotation,shift_x,shift_y = utils.estimate_global_rot_trans(\n",
    "            raw_data__[ii].squeeze().to(device), am_ref_.to(device))\n",
    "    global_rot_tran = (rotation*0,shift_x*0,shift_y*0)\n",
    "    huber_l1.global_rot_tran = global_rot_tran\n",
    "    ii = start_index+i\n",
    "    U,V = utils.pyramidal_huber_l1_optical_flow(\n",
    "            raw_data__[ii].squeeze().to(device), am_ref_.to(device),huber_l1)\n",
    "    am_prior[i] = utils.warp_2d(am_ref_, U, V)\n",
    "    am_prior[i] = bf.two_frame_histogram_matching(am_prior[i], am_ref).squeeze()\n",
    "    phase_prior[i] = utils.warp_2d(phase_ref_, U, V)\n",
    "    phase_prior[i] = bf.two_frame_histogram_matching(phase_prior[i], phase_ref).squeeze()\n",
    "am_prior = am_prior.cpu()\n",
    "am_prior_smooth = bf.temporal_gaussian_smooth_gpu(am_prior, sigma_time=0.15, kernel_size=3, device0=None).cpu()\n",
    "phase_prior = phase_prior.cpu()\n",
    "phase_prior_smooth = bf.temporal_gaussian_smooth_gpu(phase_prior, 0.15, 3).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "## Precompute LED patterns for all indices\n",
    "led_pattern_seq_set = []\n",
    "pattern_th_set = []\n",
    "raw_data_index = []\n",
    "for i_ref in tqdm(range(num_time_slot)):\n",
    "    start_ref = start_index + i_ref* frame_num\n",
    "    end_ref = start_ref + frame_num - 1\n",
    "    raw_data_index.append(start_ref)\n",
    "    raw_data_index.append(end_ref)\n",
    "    pattern_th = int(torch.ceil(torch.remainder(torch.tensor(start_ref + 1, dtype=torch.float32), \n",
    "                                            torch.tensor(n_p * frame_num, dtype=torch.float32)) / frame_num)) -1 # -1 for 0-based indexing \n",
    "    pattern_th_set.append(pattern_th)\n",
    "    pattern_mat_tmp = pattern[pattern_th]\n",
    "    led_pattern_seq = torch.nonzero(pattern_mat_tmp[0, :] != 0).squeeze()\n",
    "    led_pattern_seq_set.append(led_pattern_seq)\n",
    "    led_pattern_seq = torch.nonzero(pattern_mat_tmp[1, :] != 0).squeeze()\n",
    "    led_pattern_seq_set.append(led_pattern_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 24.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## initialize low_res object \n",
    "A_H_b_img_low_res = torch.zeros((num_time_slot*2, int(ss1*FP.mag_image), int(ss2*FP.mag_image)), dtype = torch.complex64)\n",
    "A_H_b = torch.tensor([], dtype=torch.complex64) \n",
    "for i_ref in tqdm(range(num_time_slot)):\n",
    "    start_ref = start_index + i_ref* frame_num\n",
    "    end_ref = start_ref + frame_num - 1\n",
    "    pattern_mat_tmp = pattern[pattern_th_set[i_ref]]\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref]\n",
    "    A_H_b_img_low_res[2*i_ref,:,:] = bf.FP_multiplexed(FP, raw_data_[start_ref,:,:], am_prior_smooth[2*i_ref,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref,:,:]), \n",
    "          pattern_mat_tmp[0, :]).to('cpu')\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref+1]\n",
    "    A_H_b_img_low_res[2*i_ref+1,:,:] = bf.FP_multiplexed(FP, raw_data_[end_ref,:,:], am_prior_smooth[2*i_ref+1,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref+1,:,:]), \n",
    "          pattern_mat_tmp[1, :]).to('cpu')\n",
    "A_H_b_img_abs_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.abs(A_H_b_img_low_res), 2, 5).cpu()    \n",
    "A_H_b_img_angle_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.angle(A_H_b_img_low_res), 2, 5).cpu()  \n",
    "A_H_b_img_low_res = A_H_b_img_abs_smooth * torch.exp(1j * A_H_b_img_angle_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 74.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n"
     ]
    }
   ],
   "source": [
    "## initialize mid_res object \n",
    "A_H_b_img_mid_res = torch.zeros((num_time_slot*2, int(ss1*FP.mag_image), int(ss2*FP.mag_image)), dtype = torch.complex64)\n",
    "A_H_b = torch.tensor([], dtype=torch.complex64) \n",
    "for i_ref in tqdm(range(num_time_slot)):\n",
    "    start_ref = start_index + i_ref* frame_num\n",
    "    end_ref = start_ref + frame_num - 1\n",
    "    pattern_mat_tmp = pattern[pattern_th_set[i_ref]]\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref]\n",
    "    A_H_b_img_mid_res[2*i_ref,:,:] = bf.FP_multiplexed(FP, raw_data_[start_ref,:,:], am_prior_smooth[2*i_ref,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref,:,:]), \n",
    "          pattern_mat_tmp[0, :]).to('cpu')\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref+1]\n",
    "    A_H_b_img_mid_res[2*i_ref+1,:,:] = bf.FP_multiplexed(FP, raw_data_[end_ref,:,:], am_prior_smooth[2*i_ref+1,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref+1,:,:]), \n",
    "          pattern_mat_tmp[1, :]).to('cpu')\n",
    "A_H_b_img_abs_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.abs(A_H_b_img_mid_res), 1, 3).cpu()    \n",
    "A_H_b_img_angle_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.angle(A_H_b_img_mid_res), 1, 3).cpu()  \n",
    "A_H_b_img_mid_res = A_H_b_img_abs_smooth * torch.exp(1j * A_H_b_img_angle_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 143.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n",
      "[Warning] FP_multiplexed called: this is a placeholder. Please implement your own reconstruction function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## initialize high_res object \n",
    "A_H_b_img = torch.zeros((num_time_slot*2, int(ss1*FP.mag_image), int(ss2*FP.mag_image)), dtype = torch.complex64)\n",
    "A_H_b = torch.tensor([], dtype=torch.complex64) \n",
    "\n",
    "for i_ref in tqdm(range(num_time_slot)):\n",
    "    start_ref = start_index + i_ref* frame_num\n",
    "    end_ref = start_ref + frame_num - 1    \n",
    "    pattern_mat_tmp = pattern[pattern_th_set[i_ref]]\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref]\n",
    "    A_H_b_img[2*i_ref,:,:] = bf.FP_multiplexed(FP, raw_data_[start_ref,:,:], am_prior_smooth[2*i_ref,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref,:,:]), \n",
    "          pattern_mat_tmp[0, :]).to('cpu')\n",
    "    FP.led_pattern_seq = led_pattern_seq_set[2*i_ref+1]\n",
    "    A_H_b_img[2*i_ref+1,:,:] = bf.FP_multiplexed(FP, raw_data_[end_ref,:,:], am_prior_smooth[2*i_ref+1,:,:] * torch.exp(1j*phase_prior_smooth[2*i_ref+1,:,:]), \n",
    "          pattern_mat_tmp[1, :]).to('cpu')\n",
    "A_H_b_img_abs_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.abs(A_H_b_img), 0.80, 3).cpu()    \n",
    "A_H_b_img_angle_smooth = bf.temporal_gaussian_smooth_gpu_spatial_batch(torch.angle(A_H_b_img), 0.85, 3).cpu()  \n",
    "A_H_b_img = A_H_b_img_abs_smooth * torch.exp(1j * A_H_b_img_angle_smooth)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "velocity calculation: 100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "## initialize velocity\n",
    "huber_l1 = utils.HuberL1Params(warps=10, lamda=0.05, theta=0.2, epsilon=0.04, \n",
    "                               max_iteration=10, nscales=5, sigma=0.6, zfactor=0.6, gscale=255)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "o_img_t0_abs = torch.zeros_like(A_H_b_img, dtype = torch.float32, device=device)\n",
    "with torch.no_grad():   \n",
    "    for i in range(num_time_slot*2):\n",
    "        o_img_t0_abs[i,:,:] = bf.structure_texture_decomposition_rof(torch.abs(A_H_b_img[i,:,:]))\n",
    "        o_img_t0_abs[i,:,:] = bf.histogram_transfer_gray_2d(o_img_t0_abs[i,:,:], o_img_t0_abs[0,:,:])\n",
    "U_t_abs_0 = torch.zeros_like(A_H_b_img, dtype = torch.float32, device=device)\n",
    "V_t_abs_0 = torch.zeros_like(A_H_b_img, dtype = torch.float32, device=device)\n",
    "Uaff_t_abs_0 = torch.zeros_like(A_H_b_img, dtype = torch.float32, device=device)\n",
    "Vaff_t_abs_0 = torch.zeros_like(A_H_b_img, dtype = torch.float32, device=device)\n",
    "with torch.no_grad():   \n",
    "    for i in tqdm(range(num_time_slot*2), desc = \"velocity calculation\"):\n",
    "        if i == num_time_slot*2 - 1 :\n",
    "            ref_ind = 0\n",
    "        else:\n",
    "            ref_ind = i+1    \n",
    "        rotation,shift_x,shift_y = utils.estimate_global_rot_trans(\n",
    "                o_img_t0_abs[i,:,:].squeeze().to(device), o_img_t0_abs[ref_ind,:,:].squeeze().to(device))\n",
    "        global_rot_tran = (rotation,shift_x*0,shift_y*0)\n",
    "        huber_l1.global_rot_tran = global_rot_tran\n",
    "        U_t_abs_0[i,:,:],V_t_abs_0[i,:,:] = utils.pyramidal_huber_l1_optical_flow(\n",
    "                o_img_t0_abs[i,:,:].squeeze().to(device), o_img_t0_abs[ref_ind,:,:].squeeze().to(device),huber_l1)\n",
    "        global_rot_tran = (-rotation,-shift_x*0,-shift_y*0)\n",
    "        huber_l1.global_rot_tran = global_rot_tran\n",
    "        Uaff_t_abs_0[i,:,:],Vaff_t_abs_0[i,:,:] = utils.pyramidal_huber_l1_optical_flow(\n",
    "                o_img_t0_abs[ref_ind,:,:].squeeze().to(device), o_img_t0_abs[i,:,:].squeeze().to(device),huber_l1)\n",
    "U_t_abs_0 = U_t_abs_0.cpu()\n",
    "V_t_abs_0 = V_t_abs_0.cpu()\n",
    "Uaff_t_abs_0 = Uaff_t_abs_0.cpu()\n",
    "Vaff_t_abs_0 = Vaff_t_abs_0.cpu()\n",
    "del o_img_t0_abs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize o,u,v\n",
    "o_res = A_H_b.clone()  \n",
    "o_res_image_t = torch.zeros_like(A_H_b_img, dtype = torch.complex64)\n",
    "A_H_b_tmp = A_H_b.clone()\n",
    "\n",
    "[_,s1,s2] = A_H_b_img.size()\n",
    "jo = bf.joint_optimize_Params(beta=0.1, delta1=0.5, delta2=0.1,lambda0=15, lambda1=0.1,lambda2=10, gamma=0.001,miu2=0.05, miu3=0.001)\n",
    "jo.alpha = jo.beta*huber_l1.lamda\n",
    "jo.kernel_size_am = 5\n",
    "jo.sigma_am = 1.2\n",
    "jo.kernel_size_ph = 5\n",
    "jo.sigma_ph = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res = A_H_b_img_low_res.clone().to(device)  # Low resolution\n",
    "mid_res = A_H_b_img_mid_res.clone().to(device)  # Medium resolution \n",
    "high_res = A_H_b_img.clone().to(device)  # High resolution \n",
    "U_t_abs = U_t_abs_0.clone()\n",
    "V_t_abs = V_t_abs_0.clone()\n",
    "Uaff_t_abs = Uaff_t_abs_0.clone()\n",
    "Vaff_t_abs = Vaff_t_abs_0.clone()\n",
    "o_res_image_t = torch.zeros_like(A_H_b_img, dtype = torch.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9249999523162842, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9201868772506714, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9209637641906738, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9224691390991211, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9225186109542847, Blend Weights: 0.97\n",
      "slice 1\n",
      "Epoch 1, Loss: 0.949999988079071, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9491337537765503, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9492188096046448, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9499006867408752, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9499143958091736, Blend Weights: 0.97\n",
      "slice 2\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9994457960128784, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.999548614025116, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 1.0000391006469727, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 1.0000488758087158, Blend Weights: 0.97\n",
      "slice 3\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.998950719833374, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9990466237068176, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9999077916145325, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9999227523803711, Blend Weights: 0.97\n",
      "slice 4\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9992610812187195, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9993239641189575, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9999300837516785, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9999402165412903, Blend Weights: 0.97\n",
      "slice 5\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9991343021392822, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9992047548294067, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9999133348464966, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.999925434589386, Blend Weights: 0.97\n",
      "slice 6\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9990372657775879, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9991220235824585, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9999145865440369, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9999281764030457, Blend Weights: 0.97\n",
      "slice 7\n",
      "Epoch 1, Loss: 1.0, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9993808269500732, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9994227290153503, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9999241828918457, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9999323487281799, Blend Weights: 0.97\n",
      "slice 8\n",
      "Epoch 1, Loss: 0.9749999642372131, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9741973280906677, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.974271297454834, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9749355316162109, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9749469757080078, Blend Weights: 0.97\n",
      "slice 9\n",
      "Epoch 1, Loss: 0.949999988079071, Blend Weights: 0.25\n",
      "Epoch 2, Loss: 0.9495900869369507, Blend Weights: 0.5\n",
      "Epoch 3, Loss: 0.9496799111366272, Blend Weights: 0.75\n",
      "Epoch 4, Loss: 0.9500620365142822, Blend Weights: 0.97\n",
      "Epoch 5, Loss: 0.9500695466995239, Blend Weights: 0.97\n",
      "slice 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " velocity field update: 100%|██████████| 10/10 [00:46<00:00,  4.61s/it]\n"
     ]
    }
   ],
   "source": [
    "o_res_image_t_ad = low_res.clone().detach().requires_grad_(True)\n",
    "jo = bf.joint_optimize_Params(beta=0.1, delta1=0.5, delta2=0.1,lambda0=15, lambda1=0.1,lambda2=10, gamma=0.001,miu2=0.05, miu3=0.001)\n",
    "jo.alpha = jo.beta*huber_l1.lamda\n",
    "jo.kernel_size_am = 5\n",
    "jo.sigma_am = 1.2\n",
    "jo.kernel_size_ph = 5\n",
    "jo.sigma_ph = 1.2\n",
    "upgrade_epochs = [10, 15]  # Epochs to transition towards mid and high resolutions\n",
    "\n",
    "## Optimizer\n",
    "optimizer = torch.optim.Adam([o_res_image_t_ad], lr=1e-3)  # assume no abbreviation here, one can cutomize the model when needed\n",
    "for iter in range(5):\n",
    "    for s in range(o_res_image_t_ad.shape[0]):\n",
    "        loss_norm = bf.LossNormalizer()\n",
    "        # update object \n",
    "        \n",
    "        blend_weight = 0.0    \n",
    "        num_epochs = 5  # Number of epochs\n",
    "        for epoch in range(min(int(num_epochs*(iter+1)), upgrade_epochs[1])):\n",
    "            optimizer.zero_grad() \n",
    "            # Gradually blend \n",
    "            if epoch < upgrade_epochs[0]:\n",
    "                blend_weight = min(blend_weight + 0.25, 0.97)  \n",
    "                if (iter==0) :\n",
    "                    o_res_image_t_ad.data = bf.blend_tensors(o_res_image_t_ad, mid_res, blend_weight).detach()\n",
    "                elif (iter==1) :\n",
    "                    o_res_image_t_ad.data = bf.blend_tensors(o_res_image_t_ad, high_res, blend_weight).detach()\n",
    "                o_res_image_t_ad.requires_grad = True   \n",
    "            loss_Dy = bf.apply_huber_Dy_to_3D_tensor(o_res_image_t_ad[s], jo.miu2, single_slice=True)\n",
    "            loss_Dx = bf.apply_huber_Dx_to_3D_tensor(o_res_image_t_ad[s], jo.miu2, single_slice=True)            \n",
    "            if s < jo.kernel_size_am:\n",
    "                current_kernel_size = s\n",
    "                if current_kernel_size % 2 == 0: \n",
    "                    current_kernel_size += 1\n",
    "            else:\n",
    "                current_kernel_size = jo.kernel_size_am\n",
    "            start_idx = max(s - current_kernel_size, 0)  \n",
    "            end_idx = min(s + current_kernel_size + 1, o_res_image_t_ad.shape[0])  \n",
    "            if start_idx < end_idx - 1: \n",
    "                combined_tensor = torch.cat([\n",
    "                o_res_image_t_ad[start_idx:s].detach(),  \n",
    "                o_res_image_t_ad[s:s+1],                \n",
    "                o_res_image_t_ad[s+1:end_idx].detach()  \n",
    "            ], dim=0)\n",
    "                loss_Dt = bf.apply_huber_Dt_to_3D_tensor(\n",
    "                combined_tensor, \n",
    "                jo.miu3, current_kernel_size, jo.sigma_am, current_kernel_size, jo.sigma_ph\n",
    "                )\n",
    "            else:\n",
    "                loss_Dt = torch.tensor(1e-8, device=o_res_image_t_ad.device)\n",
    "            \n",
    "            FP.led_pattern_seq = led_pattern_seq_set[s]\n",
    "            loss_fidelity = bf.FP_fidelity_loss_single_slice(FP, o_res_image_t_ad[s], raw_data_[raw_data_index[s]], device0=None)\n",
    "            \n",
    "            if s < o_res_image_t_ad.shape[0]-1:\n",
    "                combined_tensor = torch.stack([\n",
    "                o_res_image_t_ad[s],           \n",
    "                o_res_image_t_ad[s+1].detach()  \n",
    "            ], dim=0)\n",
    "                loss_flow = bf.flow_loss_single_slice(combined_tensor, U_t_abs[s], V_t_abs[s])\n",
    "            else:\n",
    "                loss_flow = torch.tensor(1e-8, device=o_res_image_t_ad.device)               \n",
    "            if s < o_res_image_t_ad.shape[0]-2 and s>0:\n",
    "                FP.led_pattern_seq = led_pattern_seq_set[s+1] \n",
    "                loss_forward_projection = bf.FP_fidelity_loss_single_slice(FP, bf.warp_2d_complex(o_res_image_t_ad[s],Uaff_t_abs[s],Vaff_t_abs[s]), raw_data_[raw_data_index[s+1]], device0=None)\n",
    "                FP.led_pattern_seq = led_pattern_seq_set[s-1] \n",
    "                loss_backward_projection = bf.FP_fidelity_loss_single_slice(FP, bf.warp_2d_complex(o_res_image_t_ad[s],U_t_abs[s-1],V_t_abs[s-1]), raw_data_[raw_data_index[s-1]], device0=None)\n",
    "                loss_projection = loss_forward_projection + loss_backward_projection\n",
    "            else:\n",
    "                loss_projection = torch.tensor(1e-8, device=o_res_image_t_ad.device)\n",
    "            loss = (0.2*loss_norm.normalize(\"Dy\", loss_Dy) +\n",
    "                        0.2*loss_norm.normalize(\"Dx\", loss_Dx) +\n",
    "                        0.05*loss_norm.normalize(\"Dt\", loss_Dt) +\n",
    "                        0.05*loss_norm.normalize(\"flow\", loss_flow) +\n",
    "                        0.45*loss_norm.normalize(\"fidelity\", loss_fidelity)+\n",
    "                        0.05*loss_norm.normalize(\"projection\", loss_projection)\n",
    "                        )\n",
    "                        \n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            \n",
    "            if epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss.item()}, Blend Weights: {blend_weight}\")\n",
    "        print(f\"slice {s + 1}\")\n",
    "            \n",
    "    # update velocity\n",
    "    huber_l1 = utils.HuberL1Params(warps=10, lamda=0.06, theta=0.2, epsilon=0.04, \n",
    "                                max_iteration=10, nscales=5, sigma=0.6, zfactor=0.6, gscale=255)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    o_img_t_abs = torch.zeros_like(o_res_image_t_ad, dtype = torch.float32, device=device)\n",
    "    with torch.no_grad():   \n",
    "        for i in range(num_time_slot*2):\n",
    "            o_img_t_abs[i,:,:] = bf.structure_texture_decomposition_rof(torch.abs(o_res_image_t_ad[i,:,:]))\n",
    "            o_img_t_abs[i,:,:] = bf.histogram_transfer_gray_2d(o_img_t_abs[i,:,:], o_img_t_abs[0,:,:])\n",
    "    U_t_abs = torch.zeros_like(o_res_image_t_ad, dtype = torch.float32, device=device)\n",
    "    V_t_abs = torch.zeros_like(o_res_image_t_ad, dtype = torch.float32, device=device)\n",
    "    Uaff_t_abs = torch.zeros_like(o_res_image_t_ad, dtype = torch.float32, device=device)\n",
    "    Vaff_t_abs = torch.zeros_like(o_res_image_t_ad, dtype = torch.float32, device=device)\n",
    "    with torch.no_grad():   \n",
    "        for i in tqdm(range(num_time_slot*2), desc=\" velocity field update\"):\n",
    "            if i == num_time_slot*2 - 1 :\n",
    "                ref_ind = 0\n",
    "            else:\n",
    "                ref_ind = i+1    \n",
    "            rotation,shift_x,shift_y = utils.estimate_global_rot_trans(\n",
    "                    o_img_t_abs[i,:,:].squeeze().to(device), o_img_t_abs[ref_ind,:,:].squeeze().to(device))\n",
    "            global_rot_tran = (rotation*0,shift_x*0,shift_y*0)\n",
    "            huber_l1.global_rot_tran = global_rot_tran\n",
    "            U_t_abs[i,:,:],V_t_abs[i,:,:] = utils.pyramidal_huber_l1_optical_flow(\n",
    "                    o_img_t_abs[i,:,:].squeeze().to(device), o_img_t_abs[ref_ind,:,:].squeeze().to(device),huber_l1)\n",
    "            global_rot_tran = (-rotation*0,-shift_x*0,-shift_y*0)\n",
    "            huber_l1.global_rot_tran = global_rot_tran\n",
    "            Uaff_t_abs[i,:,:],Vaff_t_abs[i,:,:] = utils.pyramidal_huber_l1_optical_flow(\n",
    "                    o_img_t_abs[ref_ind,:,:].squeeze().to(device), o_img_t_abs[i,:,:].squeeze().to(device),huber_l1)\n",
    "    U_t_abs = U_t_abs.cpu()\n",
    "    V_t_abs = V_t_abs.cpu()\n",
    "    Uaff_t_abs = Uaff_t_abs.cpu()\n",
    "    Vaff_t_abs = Vaff_t_abs.cpu()\n",
    "    del o_img_t_abs\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf8ea600e114af19c540b2c001bc9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='frame', max=9), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bf.interactive_video_viewer(bf.norm_rescale(o_res_image_t_ad.abs()).cpu().detach().numpy(),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f6dfc07fed4a3aaef1d2f15e5b1912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='frame', max=9), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bf.interactive_video_viewer(o_res_image_t_ad.angle().cpu().detach().numpy(),-0.8,0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test9_19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
